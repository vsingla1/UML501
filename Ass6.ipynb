{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Svarg_fbQDK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eca53082",
        "outputId": "17fcf6d8-2023-4a24-8052-98a0ce9d1f2e"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step-by-step Gaussian Naïve Bayes implementation\n",
        "\n",
        "# Function to calculate mean and variance for each feature in each class\n",
        "def calculate_class_statistics(X, y):\n",
        "    class_statistics = {}\n",
        "    for class_value in np.unique(y):\n",
        "        X_class = X[y == class_value]\n",
        "        class_statistics[class_value] = {\n",
        "            'mean': np.mean(X_class, axis=0),\n",
        "            'variance': np.var(X_class, axis=0)\n",
        "        }\n",
        "    return class_statistics\n",
        "\n",
        "# Function to calculate Gaussian probability density function (PDF)\n",
        "def gaussian_pdf(x, mean, variance):\n",
        "    # Add a small value to the variance to avoid division by zero\n",
        "    variance = variance + 1e-6\n",
        "    exponent = np.exp(-((x - mean)**2) / (2 * variance))\n",
        "    return (1 / (np.sqrt(2 * np.pi * variance))) * exponent\n",
        "\n",
        "# Function to calculate the posterior probability for each class\n",
        "def calculate_posterior_probability(x, class_statistics, prior_probabilities):\n",
        "    posteriors = {}\n",
        "    for class_value, stats in class_statistics.items():\n",
        "        prior = prior_probabilities[class_value]\n",
        "        likelihood = np.prod(gaussian_pdf(x, stats['mean'], stats['variance']))\n",
        "        posteriors[class_value] = prior * likelihood\n",
        "    return posteriors\n",
        "\n",
        "# Function to make predictions\n",
        "def predict(X, class_statistics, prior_probabilities):\n",
        "    predictions = []\n",
        "    for x in X:\n",
        "        posteriors = calculate_posterior_probability(x, class_statistics, prior_probabilities)\n",
        "        best_class = max(posteriors, key=posteriors.get)\n",
        "        predictions.append(best_class)\n",
        "    return predictions\n",
        "\n",
        "# Train the model (calculate statistics and prior probabilities)\n",
        "class_statistics = calculate_class_statistics(X_train, y_train)\n",
        "prior_probabilities = {class_value: len(y_train[y_train == class_value]) / len(y_train) for class_value in np.unique(y_train)}\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "y_pred = predict(X_test, class_statistics, prior_probabilities)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Custom Gaussian Naïve Bayes Accuracy: {accuracy}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Gaussian Naïve Bayes Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ff85e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The custom Gaussian Naïve Bayes model has been implemented, trained, and evaluated. The next step is to implement the Gaussian Naïve Bayes classifier using the in-built function from scikit-learn and compare its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0020b186",
        "outputId": "39313bfb-e208-41d8-bfca-57aefbbced4d"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# In-built Gaussian Naïve Bayes implementation\n",
        "\n",
        "# Initialize and train the in-built model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_inbuilt = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_inbuilt = accuracy_score(y_test, y_pred_inbuilt)\n",
        "print(f\"In-built Gaussian Naïve Bayes Accuracy: {accuracy_inbuilt}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-built Gaussian Naïve Bayes Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba565507"
      },
      "source": [
        "## Explore gridsearchcv for k-nn\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable dataset for K-NN (e.g., the Iris dataset), split the data into training and testing sets, define a parameter grid for the number of neighbors (K) for K-NN, use `GridSearchCV` to find the best value of K, train a K-NN model with the best K found by GridSearchCV, and evaluate the K-NN model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2f14cd"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the Iris dataset and split it into training and testing sets as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75478d7d"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "623f7a0e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary modules, instantiate KMeansClassifier, define the parameter grid, and instantiate GridSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0aed9a9"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Instantiate a KNeighborsClassifier object\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define a dictionary named param_grid with a single key 'n_neighbors'\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}\n",
        "\n",
        "# Instantiate a GridSearchCV object\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba88fbce"
      },
      "source": [
        "**Reasoning**:\n",
        "Fit the GridSearchCV object to the training data and print the best value of K.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5243c90",
        "outputId": "bc81df8d-c5ad-43b7-d665-cee1e01e055d"
      },
      "source": [
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best value of K found by GridSearchCV\n",
        "print(f\"Best K value found by GridSearchCV: {grid_search.best_params_['n_neighbors']}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K value found by GridSearchCV: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5faf13d"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a K-NN model with the best K found by GridSearchCV, make predictions on the test set, and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1edf7ed",
        "outputId": "e6cd45b0-0506-4a89-ab60-a7bf6da631db"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train a K-NN model with the best K value\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = knn_best.predict(X_test)\n",
        "\n",
        "# Evaluate the performance by calculating the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of K-NN model with best K on the test set: {accuracy}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of K-NN model with best K on the test set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7c1150"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Both the custom and the in-built Gaussian Naïve Bayes implementations achieved perfect accuracy (1.0) on the Iris dataset test set.\n",
        "*   Using `GridSearchCV`, the best value for the `n_neighbors` parameter (K) for the K-NN classifier on the Iris dataset was found to be 3.\n",
        "*   A K-NN model trained with the best K (K=3) achieved an accuracy of 1.0 on the test set.\n",
        "\n"
      ]
    }
  ]
}